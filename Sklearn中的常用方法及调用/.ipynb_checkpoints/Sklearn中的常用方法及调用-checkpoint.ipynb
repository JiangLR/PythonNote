{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.分类\n",
    "1. from sklearn.model_selection import train_test_split  \n",
    "2. X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.KNN算法\n",
    "1. from sklearn.neighbors import KNeighborsClassifier\n",
    "2. kNN_classifier = KNeighborsClassifier(n_neighbors=6)\n",
    "3. kNN_classifier.fit(X_train, y_train)\n",
    "    * y_predict = kNN_classifier.predict(X_test)   预测值\n",
    "    * knn_clf.score(X_test, y_test)   准确度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Grid Search(采用了交叉验证)\n",
    "1.\n",
    "```\n",
    "param_grid = [\n",
    "    {\n",
    "        'weights': ['uniform'], \n",
    "        'n_neighbors': [i for i in range(1, 11)]\n",
    "    },\n",
    "    {\n",
    "        'weights': ['distance'],\n",
    "        'n_neighbors': [i for i in range(1, 11)], \n",
    "        'p': [i for i in range(1, 6)]\n",
    "    }\n",
    "]\n",
    "```\n",
    "2. knn_clf = KNeighborsClassifier()\n",
    "3. from sklearn.model_selection import GridSearchCV\n",
    "4. grid_search = GridSearchCV(knn_clf, param_grid, cv=5)  ## cv是交叉验证份数，可省略\n",
    "5. grid_search.fit(X_train, y_train)\n",
    "    * grid_search.best_estimator_\n",
    "        1. best_knn_clf = grid_search.best_estimator_\n",
    "        2. best_knn_clf.score(X_test, y_test)\n",
    "    * grid_search.best_score_\n",
    "    * grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.归一化(特征缩放)\n",
    "1. from sklearn.preprocessing import StandardScaler\n",
    "2. standardScalar = StandardScaler() \n",
    "3. standardScalar.fit(X_train)\n",
    "    * standardScalar.mean_   (均值)\n",
    "    * standardScalar.scale_   (标准差)\n",
    "4. X_train_standard = standardScalar.transform(X_train)   (对X_train进行归一化)\n",
    "5. 以KNN举例 knn_clf.fit(X_train_standard, y_train)\n",
    "6. knn_clf.score(X_test_standard, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.数据集导入(以鸢尾花数据集为例)\n",
    "1. from sklearn import datasets\n",
    "2. iris = datasets.load_iris()\n",
    "    * iris.keys()   # 输出：dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])\n",
    "    * print(iris.DESCR)\n",
    "    * iris.feature_names\n",
    "3. X = iris.data\n",
    "4. y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.MSE(均方误差)和MAE(平均绝对误差)和MRE(均方根误差)和R²，sklearn中个关于指标的方法都放在sklearn.metrics包中\n",
    "from sklearn.metrics import mean_squared_error  \n",
    "from sklearn.metrics import mean_absolute_error  \n",
    "from sklearn.metrics import root_mean_squared_error  \n",
    "from sklearn.metrics import r2_score  \n",
    "* r2_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.线性回归(多元，简单，梯度下降)\n",
    "1. from sklearn.linear_model import LinearRegression\n",
    "2. lin_reg = LinearRegression()\n",
    "3. lin_reg.fit(X_train, y_train)\n",
    "    * lin_reg.coef_\n",
    "    * lin_reg.intercept_\n",
    "    * lin_reg.score(X_test, y_test)   # R²值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.SGD(随机梯度下降)\n",
    "只能解决线性模型\n",
    "1. from sklearn.linear_model import SGDRegressor\n",
    "2. sgd_reg = SGDRegressor(n_iter=50)\n",
    "3. sgd_reg.fit(X_train_standard, y_train)\n",
    "    * sgd_reg.score(X_test_standard, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.PCA(梯度上升，特征压缩)\n",
    "1. from sklearn.decomposition import PCA\n",
    "2. pca = PCA(n_components=1)   # n_components表示特征维度\n",
    "    * pca = PCA(0.95)  # 表示不知道要取几个维度，但是取的主成分个数能解释原数据95%的方差\n",
    "3. pca.fit(X)\n",
    "    * pca.components_\n",
    "    * pca.n_components_  # 通过pca = PCA(0.95)得出的主成分个数\n",
    "4. X_reduction = pca.transform(X)\n",
    "5. X_restore = pca.inverse_transform(X_reduction)\n",
    "#### 与其他数据预测算法结合(以KNN算法为例)\n",
    "    1. X_train_reduction = pca.transform(X_train)\n",
    "    2. X_test_reduction = pca.transform(X_test)\n",
    "    3. knn_clf = KNeighborsClassifier()\n",
    "    4. knn_clf.fit(X_train_reduction, y_train)\n",
    "        * knn_clf.score(X_test_reduction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.多项式回归(PCA和多项式回归都是对现有X数据进行变形)\n",
    "1. from sklearn.preprocessing import PolynomialFeatures\n",
    "2. poly = PolynomialFeatures(degree=n)   # 为原本的数据集最多添加n次幂这样的特征,sklearn中会自动添加零次幂\n",
    "3. poly.fit(X)\n",
    "4. X2 = poly.transform(X)   # 把X转换成多项式X2\n",
    "#### 与线性回归算法结合(多项式回归只是预处理过程(sklearn.preprocessing)，真正拟合还是得用线性回归算法)\n",
    "    1. from sklearn.linear_model import LinearRegression\n",
    "    2. lin_reg2 = LinearRegression()\n",
    "    3. lin_reg2.fit(X2, y)\n",
    "        * y_predict2 = lin_reg2.predict(X2)\n",
    "        * lin_reg2.score(X2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.PipeLine(管道)\n",
    "```\n",
    "1. 多项式的特征(或者其他预处理算法)\n",
    "2. 数据的归一化(或者其他预处理算法)(对于多项式回归，数据标准化是必要的，因为如果超参数degree很大的话，数据之间的差距会很大，比如1的一次方和100的100次方之间的差距)\n",
    "3. 线性回归(或者其他拟合算法)\n",
    "...\n",
    "PipeLine将三步合在一起\n",
    "```\n",
    "1. from sklearn.pipeline import Pipeline\n",
    "2. from sklearn.preprocessing import StandardScaler\n",
    "3. from sklearn.linear_model import LinearRegression\n",
    "4.  \n",
    "```\n",
    "poly_reg = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(degree=2)),\n",
    "    (\"std_scaler\", StandardScaler()),\n",
    "    (\"lin_reg\", LinearRegression())\n",
    "])\n",
    "```\n",
    "5. poly_reg.fit(X, y)\n",
    "    * y_predict = poly_reg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.使用交叉验证\n",
    "1. from sklearn.model_selection import cross_val_score\n",
    "2. knn_clf = KNeighborsClassifier()\n",
    "3. cross_val_score(knn_clf, X_train, y_train, cv=5) ## cv是交叉验证份数，可省略\n",
    "    * 返回k个模型，每个模型的准确度组成的数组\n",
    "    * cross_val_score默认分成三份进行交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.岭回归\n",
    "1. from sklearn.linear_model import Ridge\n",
    "2. ridge = Ridge(alpha=1)\n",
    "#### 使用管道\n",
    "ridge1_reg = Pipeline([\n",
    "        (\"poly\", PolynomialFeatures(degree=20)),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "        (\"ridge_reg\", Ridge(alpha=0.0001))\n",
    "    ])\n",
    "3. ridge1_reg.fit(X_train, y_train)\n",
    "4. y1_predict = ridge1_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.LASSO回归\n",
    "1. from sklearn.linear_model import Lasso\n",
    "2. lasso = Lasso(alpha=1)\n",
    "#### 使用管道\n",
    "lasso1_reg = Pipeline([\n",
    "        (\"poly\", PolynomialFeatures(degree=20)),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "        (\"ridge_reg\", Lasso(alpha=0.1))\n",
    "    ])\n",
    "3. lasso1_reg.fit(X_train, y_train)\n",
    "4. y1_predict = lasso1_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.逻辑回归\n",
    "1. from sklearn.linear_model import LogisticRegression\n",
    "2. log_reg = LogisticRegression()\n",
    "3. log_reg.fit(X_train, y_train)\n",
    "    * log_reg.score(X_train, y_train)\n",
    "#### 虽然逻辑回归只能解决二分类问题，但是sklearn中自动添加了支持多分类任务的功能，默认采用OVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.OVO and OVR\n",
    "\n",
    "1. from sklearn.multiclass import OneVsRestClassifier\n",
    "    * from sklearn.multiclass import OneVsOneClassifier\n",
    "2. log_reg = LogisticRegression()\n",
    "3. ovr = OneVsRestClassifier(log_reg)\n",
    "    * ovo = OneVsOneClassifier(log_reg)\n",
    "4. ovr.fit(X_train, y_train)\n",
    "5. ovr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.逻辑回归中的交叉验证\n",
    "1. from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "2. log_reg_cv = LogisticRegressionCV()\n",
    "3. log_reg_cv.fit(X_train, y_train)\n",
    "    * log_reg_cv.score(X_test, y_test)\n",
    "    * log_reg_cv.C_  \n",
    "    array([ 0.00599484,  0.00599484,  0.04641589,  0.35938137,  0.00599484,\n",
    "        0.35938137,  0.35938137,  2.7825594 ,  0.00599484,  0.04641589])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.混淆矩阵(默认支持多分类问题)，精准率和召回率和F1-Score\n",
    "1. from sklearn.metrics import confusion_matrix\n",
    "    * from sklearn.metrics import precision_score(精准率)\n",
    "    * from sklearn.metrics import recall_score(召回率)\n",
    "    * from sklearn.metrics import f1_score\n",
    "2. log_reg = LogisticRegression()\n",
    "3. log_reg.fit(X_train, y_train)\n",
    "4. y_log_predict = log_reg.predict(X_test)\n",
    "    * confusion_matrix(y_test, y_log_predict)\n",
    "    * precision_score(y_test, y_log_predict)\n",
    "    * recall_score(y_test, y_log_predict)\n",
    "    * f1_score(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.精准度和召回率的平衡(sklearn中逻辑回归的方法decision_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 在逻辑回归中，判断样本是属于分类1还是分类2的依据值  \n",
    "2. 分数小于0则被判断为类别1  \n",
    "3. 分数大于0则被判断为类别2\n",
    "```\n",
    "decision_scores = log_reg.decision_function(X_test)\n",
    "```\n",
    "array([-22.05700117, -33.02940957, -16.21334087, -80.3791447 , -48.25125396, -24.54005629, -44.39168773,  -25.04292757, -0.97829292, -19.7174399 ])  \n",
    "##### 数组里的值表示每个样本点在score数轴上对应的score值  \n",
    "```\n",
    "y_predict_1 = log_reg.predict(X_test)[:10]\n",
    "```\n",
    "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])  \n",
    "4. 改变分类依据，现在是分数大于5，则判断为类1  \n",
    "5. 分数小于5，则判断为类2\n",
    "```\n",
    "y_predict_2 = np.array(decision_scores >= 5, dtype='int')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. scikit-learn中的Precision-Recall曲线\n",
    "1. from sklearn.metrics import precision_recall_curve\n",
    "2. precisions, recalls, thresholds = precision_recall_curve(y_test, decision_scores)\n",
    "    * plt.plot(thresholds, precisions[:-1])\n",
    "    * plt.plot(thresholds, recalls[:-1])\n",
    "    * plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. scikit-learn中的ROC，AUC\n",
    "1. from sklearn.metrics import roc_curve\n",
    "    * from sklearn.metrics import roc_auc_score\n",
    "2. fprs, tprs, thresholds = roc_curve(y_test, decision_scores)\n",
    "3. roc_auc_score(y_test, decision_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. scikit-learn中的SVM\n",
    "#### 先进行标准化\n",
    "1. from sklearn.preprocessing import StandardScaler\n",
    "2. standardScaler = StandardScaler()\n",
    "3. standardScaler.fit(X)\n",
    "4. X_standard = standardScaler.transform(X)\n",
    "#### 再进行svm分类\n",
    "5. from sklearn.svm import LinearSVC\n",
    "6. svc = LinearSVC(C=1e9)\n",
    "7. svc.fit(X_standard, y)\n",
    "#### 这时候svc训练出了一个决策边界，可以对输入数据进行分类\n",
    "8. y_predict = svc.predict(X_test)\n",
    "##### 这时就可以进行一些操作，比如精准率召回率的判断\n",
    "    * from sklearn.metrics import f1_score\n",
    "    * f1_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. 使用多项式特征的SVM\n",
    "1. from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "2. from sklearn.svm import LinearSVC\n",
    "3. from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef PolynomialSVC(degree, C=1.0):\\n    return Pipeline([\\n        (\"poly\", PolynomialFeatures(degree=degree)),\\n        (\"std_scaler\", StandardScaler()),\\n        (\"linearSVC\", LinearSVC(C=C))\\n    ])\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def PolynomialSVC(degree, C=1.0):\n",
    "    return Pipeline([\n",
    "        (\"poly\", PolynomialFeatures(degree=degree)),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "        (\"linearSVC\", LinearSVC(C=C))\n",
    "    ])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. poly_svc = PolynomialSVC(degree=3)\n",
    "5. poly_svc.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25. scikit-learn中的决策树\n",
    "1. from sklearn.tree import DecisionTreeClassifier\n",
    "##### criterion=\"entropy\"：决策树划分标准为信息熵方式\n",
    "2. dt_clf = DecisionTreeClassifier(max_depth=2, criterion=\"entropy\", random_state=42)  ## criterion=\"gini\"\n",
    "3. dt_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26. 决策树解决回归问题\n",
    "1. from sklearn.tree import DecisionTreeRegressor\n",
    "2. dt_reg = DecisionTreeRegressor()  ## 可以进行超参数的调试\n",
    "3. dt_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27.集成学习Voting Classifier\n",
    "1. from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n### voting='hard'表示'少数服从多数'这个集成学习方式\\n### 在具体使用时，可以先调整这些分类器的参数，把每个算法调整到最好的情况，再一起使用\\n2. voting_clf = VotingClassifier(estimators=[\\n    ('log_clf', LogisticRegression()), \\n    ('svm_clf', SVC()),\\n    ('dt_clf', DecisionTreeClassifier(random_state=666))],\\n                             voting='hard')\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "### voting='hard'表示'少数服从多数'这个集成学习方式\n",
    "### 在具体使用时，可以先调整这些分类器的参数，把每个算法调整到最好的情况，再一起使用\n",
    "2. voting_clf = VotingClassifier(estimators=[\n",
    "    ('log_clf', LogisticRegression()), \n",
    "    ('svm_clf', SVC()),\n",
    "    ('dt_clf', DecisionTreeClassifier(random_state=666))],\n",
    "                             voting='hard')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. voting_clf.fit(X_train, y_train)\n",
    "    * voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nvoting_clf2 = VotingClassifier(estimators=[\\n    ('log_clf', LogisticRegression()), \\n    ('svm_clf', SVC(probability=True)),   对于SVC算法，必须要设置probability=True才能计算概率\\n    ('dt_clf', DecisionTreeClassifier(random_state=666))],\\n                             voting='soft')\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "voting_clf2 = VotingClassifier(estimators=[\n",
    "    ('log_clf', LogisticRegression()), \n",
    "    ('svm_clf', SVC(probability=True)),   对于SVC算法，必须要设置probability=True才能计算概率\n",
    "    ('dt_clf', DecisionTreeClassifier(random_state=666))],\n",
    "                             voting='soft')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28. 使用Bagging进行集成学习\n",
    "1. from sklearn.tree import DecisionTreeClassifier\n",
    "2. from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbagging_clf = BaggingClassifier(DecisionTreeClassifier(),\\n                           n_estimators=5000, max_samples=100,\\n                           bootstrap=True)\\n                           \\nbagging_clf.fit(X_train, y_train)\\nbagging_clf.score(X_test, y_test)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## 这里的DecisionTreeClassifier()只是采用了一个决策树算法，使用决策树算法只是因为这种非参数算法更能产生出差异非常大的子模型\n",
    "## 这个位置可以替代为通过Voting-Classifier聚合各种算法生成的模型\n",
    "## n_estimators表示要集成多少个子模型， max_samples表示每个子模型要看多少个样本数据，bootstrap=True表示放回取样的方式\n",
    "3. bagging_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                           n_estimators=5000, max_samples=100,\n",
    "                           bootstrap=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. bagging_clf.fit(X_train, y_train)\n",
    "5. bagging_clf.score(X_test, y_test)\n",
    "\n",
    "#### OOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## oob_score=True:记录在放回取样的郭恒中都取了哪些样本，哪些样本没有被取到，后续才能调用oob_score_\\nbagging_clf = BaggingClassifier(DecisionTreeClassifier(),\\n                           n_estimators=5000, max_samples=100,\\n                           bootstrap=True)\\n## 有了oob则不需要进行train_test_split\\nbagging_clf.fit(X, y)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## oob_score=True:记录在放回取样的郭恒中都取了哪些样本，哪些样本没有被取到，后续才能调用oob_score_\n",
    "1. bagging_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                           n_estimators=5000, max_samples=100,\n",
    "                           bootstrap=True)\n",
    "## 有了oob则不需要进行train_test_split\n",
    "2. bagging_clf.fit(X, y)\n",
    "## 测试结果\n",
    "3. bagging_clf.oob_score_\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bootstrap_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## max_features：最大看几个特征,此时bootstrap_features=True\n",
    "## 如果样本空间特征少的话使用随即特征采样的方式是不合适的\n",
    "## 此时n_estimators=600, max_samples=500，因为不对样本进行随机采样(前提条件是此时一共有500个样本)\n",
    "## 如果n_estimators=600, max_samples=100，就表示既对样本数进行放回采样，又对特征数放回采样\n",
    "random_subspaces_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                               n_estimators=600, max_samples=500,\n",
    "                               bootstrap=True, oob_score=True,\n",
    "                               max_features=1, bootstrap_features=True)\n",
    "random_subspaces_clf.fit(X, y)\n",
    "random_subspaces_clf.oob_score_\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 29. sklearn中的随机森林\n",
    "#### 随机森林拥有决策树和BaggingClassifier的所有参数\n",
    "1. from sklearn.ensemble import RandomForestClassifier\n",
    "2. rf_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, oob_score=True, random_state=666, n_jobs=-1)\n",
    "3. rf_clf.fit(X, y)\n",
    "    * rf_clf.oob_score_  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30. 集成学习解决回归问题\n",
    "1. from sklearn.ensemble import BaggingRegressor\n",
    "2. from sklearn.ensemble import RandomForestRegressor\n",
    "3. from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 31. AdaBoosting\n",
    "1. from sklearn.tree import DecisionTreeClassifier\n",
    "2. from sklearn.ensemble import AdaBoostClassifier\n",
    "3. ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=500)\n",
    "4. ada_clf.fit(X_train, y_train)\n",
    "5. ada_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32. Gradient Boosting\n",
    "1. from sklearn.ensemble import GradientBoostingClassifier\n",
    "2. gb_clf = GradientBoostingClassifier(max_depth=2, n_estimators=30)\n",
    "3. gb_clf.fit(X_train, y_train)\n",
    "4. gb_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 33. Boosting 解决回归问题\n",
    "1. from sklearn.ensemble import AdaBoostRegressor\n",
    "2. from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
