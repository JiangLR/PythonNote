{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "1. 数据爬取的主要目的就是从非结构性的数据源来提取到结构性的数据。比如说网页，它是一个非结构性数据，我们把很多网站上的网页获取下来之后，把它们结构化成我们自己定义的结构。\n",
    "2. 但是当我们提取了这些数据之后如何将数据返回：最简单的是把返回的数据放入一个字典当中，通过字典返回给Scrapy\n",
    "3. 虽然字典(dict)很好用，但是dict缺乏一些结构性的元素，比如容易打错字段的名字，在统一处理字典时容易出错 \n",
    "```\n",
    "爬虫A返回字典\n",
    "A:\n",
    "{\n",
    "    'fav_nums':\n",
    "    'comment_nums:'\n",
    "}\n",
    "爬虫B返回字典\n",
    "B:\n",
    "{\n",
    "    'fav_num':\n",
    "    'comment_num'\n",
    "}\n",
    "```\n",
    "4. 为了进行完整的格式化，Scrapy提供了Item类(items.py)，Item类允许用户自定义字段，比如自定义'Article'Item类，在Article统一定义各种爬虫所共有的字段\n",
    "5. 当我们队Item进行实例化之后，在对应的py文件里进行yield操作时，比如通过parse()获得了一个Item类，然后直接把Item类yield就行了，Scrapy在接受yield时，发现这是一个Item实例的时候，会直接将该Item路由到'piplines.py'文件里\n",
    "    * 好处：在'piplines.py'文件里集中处理数据的保存,去重...\n",
    "```\n",
    "def parse(self, response):  \n",
    "        post_urls = response.css(\"#archive .floated-thumb .post-thumb a::attr(href)\").extract()\n",
    "        for post_url in post_urls:\n",
    "            yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *拓展延伸\n",
    "详情页：<img src=\"1.png\" width='400'>\n",
    "列表页：<img src=\"2.png\" width='400'>\n",
    "* 因为对于Item类的处理是在parse_detail()方法中定义并且进行的，如果我们想在Item中保存对应文章列表页的图片，需要将列表页的图片传递到parse_detail()进行处理\n",
    "* 通过Request()方法中的meta参数进行传参，meta参数是一个dict(因为我们不仅可以只传递图片，还可以传递任意我们想要的参数，这些参数通过dict进行保存)\n",
    "    ```\n",
    "    post_nodes = response.css(\"#archive .floated-thumb .post-thumb a\")\n",
    "        for post_node in post_nodes:\n",
    "            image_url = post_node.css(\"img::attr(src)\").extract_first(\"\")\n",
    "            post_url = post_node.css(\"::attr(href)\").extract_first(\"\")\n",
    "            yield Request(url=parse.urljoin(response.url, post_url), meta={\"front_image_url\": image_url}\n",
    "                ,callback=self.parse_detail)\n",
    "    ```\n",
    "<img src='3.png'>\n",
    "* 然后再parse_details()用变量接受meta参数\n",
    "```\n",
    "def parse_detail(self, response):\n",
    "        # response.meta.get(\"front_image_url\", \"\")是一种不会抛异常的方法，本质和response.meta['front_image_url']相同\n",
    "        front_image_url = response.meta.get(\"front_image_url\", \"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## item.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 区别于其他数据类型(int, string...), 在Item类中只有一个字段类型：scrapy.Field()\n",
    "```\n",
    "class JobBoleArticleIte(scrapy.Item):\n",
    "        # 标题\n",
    "        title = scrapy.Field()\n",
    "        # 创建时间\n",
    "        create_date = scrapy.Field()\n",
    "        # 当前页面的url\n",
    "        url = scrapy.Field()\n",
    "        # URL对应的MD5\n",
    "        url_object_id = scrapy.Field()\n",
    "        # 封面图\n",
    "        front_image_url = scrapy.Field()\n",
    "        # 封面图本地存放路径\n",
    "        front_image_path = scrapy.Field()\n",
    "        # 点赞数\n",
    "        praise_nums = scrapy.Field()\n",
    "        # 评论数\n",
    "        comment_nums = scrapy.Field()\n",
    "        # 收藏数\n",
    "        fav_nums = scrapy.Field()\n",
    "        # 标签\n",
    "        tags = scrapy.Field()\n",
    "        # 内容\n",
    "        content = scrapy.Field()\n",
    "        pass\n",
    "```\n",
    "2. 填充Item类中对应的字段\n",
    "    * 首先需要在py文件中导入Item类\n",
    "        * from ArticleSpider.items import JobBoleArticleItem\n",
    "    * 实例化Item类\n",
    "        * article_item = JobBoleArticleItem()\n",
    "    * 填充Item对象\n",
    "        * article_item[\"create_date\"] = create_date......\n",
    "    * 调用yield() # Scrapy在接受yield时，发现这是一个Item实例的时候，会直接将该Item路由到'piplines.py'文件里\n",
    "        * yield article_item\n",
    "    * 使Pipline生效，从而能够处理Item\n",
    "        ```\n",
    "        'settings.py'\n",
    "        # Configure item pipelines\n",
    "        # See https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "        ITEM_PIPELINES = {\n",
    "            'ArticleSpider.pipelines.ArticlespiderPipeline': 300,\n",
    "        }\n",
    "        ```\n",
    "<img src='4.png'>\n",
    "3. 给Item类中的字段添加功能(保存到本地......)\n",
    "    * scrapy提供了一种自动下载图片的机制，只需要在'settings.py'进行配置\n",
    "        * 管道(PIPELINES)对象:数字表示数据在管道中流经的顺序,即处理类对数据处理的顺序：小->大,先->后\n",
    "        ```\n",
    "        ITEM_PIPELINES = {\n",
    "                'ArticleSpider.pipelines.ArticlespiderPipeline': 300,\n",
    "                'scrapy.piplines.images.ImagesPipeline': 1\n",
    "        }\n",
    "        ```\n",
    "        * 配置Item类中哪个字段代表的是图片，然后PIPLINE到Item中去取对应图片的字段\n",
    "        ```\n",
    "        ITEM_PIPELINES = {\n",
    "                'ArticleSpider.pipelines.ArticlespiderPipeline': 300,\n",
    "                'scrapy.piplines.images.ImagesPipeline': 1\n",
    "                }\n",
    "        IMAGE_URLS_FIELD = 'front_image_url'\n",
    "        ```\n",
    "        * 配置图片下载路径\n",
    "            * 这里有必要对路径的表示进行说明 \n",
    "                * os.path.abspath(os.path.dirname(\\__file\\__))   \n",
    "                ->G:\\Python\\PythonNote\\Scrapy\\ArticleSpider\n",
    "                * os.path.dirname(\\__file\\__))  \n",
    "                ->G:/Python/PythonNote/Scrapy/ArticleSpider\n",
    "                * os.path.dirname(os.path.abspath(\\__file\\__))  \n",
    "                ->G:\\Python\\PythonNote\\Scrapy\\ArticleSpider\n",
    "            * os.path.dirname()只是获取目录名字,而不是目录路径,在windows下反斜杠'\\'才表示路径\n",
    "            * os.path.abspath()才是获取文件目录的地址\n",
    "            ```\n",
    "            ITEM_PIPELINES = {\n",
    "                'ArticleSpider.pipelines.ArticlespiderPipeline': 300,\n",
    "                'scrapy.piplines.images.ImagesPipeline': 1\n",
    "            }\n",
    "            IMAGES_URLS_FIELD = 'front_image_url'\n",
    "\n",
    "            import os\n",
    "\n",
    "            project_dir = os.path.abspath(os.path.dirname(__file__))\n",
    "            IMAGES_STORE = os.path.join(project_dir, 'images')\n",
    "            ```\n",
    "        * 下载处理图片的相关库(PIL)\n",
    "            * pip install -i https://pypi.douban.com/simple pillow\n",
    "        * 处理异常：raise ValueError('Missing scheme in request url: %s' % self._url)\n",
    "            * 在PIPLINE处理图片时,IMAGE_URLS_FIELD被PIPLINE默认为数组类型，所以需要在相关py文件下把front_image_url设置为数组\n",
    "            * article_item[\"front_image_url\"] = [front_image_url]\n",
    "<img src='5.png'>\n",
    "    * 图片既然已经保存下来了，就需要把图片的路径提取出来，然后把图片路径和front_image_path字段绑定在一起，以便之后进行存入数据库操作\n",
    "    * 对于图片下载有与之对应的'scrapy.piplines.images.ImagesPipeline'类可以进行操作，但是把图片路径和front_image_path绑定在一起的处理类并没有现成的，\n",
    "    所以需要自定义一个能够处理对应功能的PIPLINE类\n",
    "    #### (拓展)自定义PIPLINE类的编写方式：\n",
    "         * 注意重载\n",
    "         * 注意返回item,因为处于管道后面的管道类同样需要处理item，如果不返回item，则处于序列后面的PIPELINE类就没有item可以处理了\n",
    "    * 自定义的PIPELINE类需要继承ImagesPipeline里的部分功能重载，在此基础上自定义其他功能\n",
    "        ```\n",
    "            # 观察这个方法体的循环，就知道为什么我们需要把front_image_url设为数组形式的数据了\n",
    "            # 获得数组元素后，通过Request传递给scrapy进行下载\n",
    "            def get_media_requests(self, item, info):\n",
    "                return [Request(x) for x in item.get(self.images_urls_field, [])]\n",
    "        ```\n",
    "        ```\n",
    "            # 重载这个方法之后就可以获得到文件的下载路径，路径存放在results参数里\n",
    "            def item_completed(self, results, item, info):\n",
    "                if isinstance(item, dict) or self.images_result_field in item.fields:\n",
    "                    item[self.images_result_field] = [x for ok, x in results if ok]\n",
    "                return item\n",
    "        ```\n",
    "        <img src='6.png'>\n",
    "        ```\n",
    "        from scrapy.pipelines.images import ImagesPipeline\n",
    "        \n",
    "        class ArticleImagePipLine(ImagesPipeline):\n",
    "            def item_completed(self, results, item, info):\n",
    "                # 通过results获得文件实际的存储路径\n",
    "                for ok, value in results:\n",
    "                    image_file_path = value['path']\n",
    "                item['front_image_path'] = image_file_path\n",
    "\n",
    "                return item\n",
    "        ```\n",
    "        * results[1].path:文件存放路径\n",
    "        * results[1].url:文件url\n",
    "        * results[0]:下载状态(是否成功)  \n",
    "    * 下一个PIPLINE接受数据(成功接收到front_image_path)\n",
    "    <img src='7.png'>\n",
    "    * 获取到数据之后就可以进行保存数据库...操作了\n",
    "4. 把不定长的url地址转换成定长的md5编码\n",
    "    * 新建相应的方法(get_md5)\n",
    "    ```\n",
    "    import hashlib\n",
    "    def get_md5(url):\n",
    "        if isinstance(url, str):\n",
    "            url = url.encode('utf-8')\n",
    "        m = hashlib.md5()\n",
    "        m.update(url)\n",
    "        return m.hexdigest()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scrapy",
   "language": "python",
   "name": "scrapy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
