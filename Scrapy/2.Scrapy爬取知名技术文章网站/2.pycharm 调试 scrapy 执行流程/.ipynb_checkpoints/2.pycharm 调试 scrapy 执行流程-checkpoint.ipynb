{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class JobboleSpider(scrapy.Spider):\n",
    "    name = 'jobbole'\n",
    "    allowed_domains = ['blog.jobbole.com']\n",
    "    start_urls = ['http://blog.jobbole.com/']\n",
    "    \n",
    "    # 每次爬取start_urls里的url时，会执行parse里的方法\n",
    "    def parse(self, response):\n",
    "        pass\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 因为JobboleSpider类是继承自scrapy.Spider,所以会默认执行scrapy.Spider已经定义的方法\n",
    "2. scrapy.Spider中有**start_requests()**\n",
    "```\n",
    "def start_requests(self):\n",
    "    for url in self.start_urls:\n",
    "        yield Request(url, dont_filter=True)\n",
    "```\n",
    "start_requests()会向目的地址发送请求,返回一个Request可迭代对象\n",
    "3. 如果response没有指定回调函数(callback),即Request()没有指定callback(), parse()会被默认调用,它负责处理Response,处理返回结果,并从中提取出想要的数据和下一步的请求,然后返回.该方法需要返回一个包含**Request**或**Item**的可迭代对象(自定义的callback函数也必须返回一个包含**Request**或**Item**的可迭代对象)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 因为pyCharm没有Scrapy模板，所以无法调试,我们可以自创建一个main文件，通过运行main文件来调试爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom scrapy.cmdline import execute\\n\\nimport sys\\nimport os\\n\\n# print(os.path.dirname(os.path.abspath(__file__)))\\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\\nexecute(['scrapy', 'crawl', 'jobbole'])\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### main文件内容\n",
    "'''\n",
    "from scrapy.cmdline import execute\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# print(os.path.dirname(os.path.abspath(__file__)))\n",
    "# 输出 -> G:\\Python\\Scrapy\\ArticleSpider\n",
    "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "execute(['scrapy', 'crawl', 'jobbole'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 在Scrapy目录下创建main文件\n",
    "2. 通过execute函数来调用爬虫脚本 \n",
    "3. sys.path.append()来设置工程目录，否则调用execute函数将无法生效\n",
    "    * 代码为：sys.path.append(G:\\Python\\Scrapy\\ArticleSpider)，但是可能运行机器不同所以需要把地址换成当前main文件所在路径\n",
    "    * os.path.abspath(\\__file\\__) 获取的是当前main文件的路径，但是我们要的是它的父目录\n",
    "    * sys.path.append(os.path.dirname(os.path.abspath(\\__file\\__)))\n",
    "4. 调用execute函数执行scrpay命令\n",
    "    * scrapy启动某个爬虫的命令为 scrapy crawl 爬虫工程名字    //(name = 'jobbole'， 那么名字就为'jobbole')\n",
    "    * execute(['scrapy', 'crawl', 'jobbole'])\n",
    "5. 在settings文件里修改参数，把ROBOTSTXT_OBEY的值改为False\n",
    "    ```\n",
    "        # Obey robots.txt rules\n",
    "        ROBOTSTXT_OBEY = False\n",
    "    ```\n",
    "    * 如果不设置ROBOTSTXT_OBEY为False的话，scrapy就会默认去读取网站上的ROBOTS协议，他会把不符合ROBOTS协议的url给过滤掉\n",
    "    \n",
    "6. 给parse函数方法体打上断点，debug main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='1.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scrapy",
   "language": "python",
   "name": "scrapy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
