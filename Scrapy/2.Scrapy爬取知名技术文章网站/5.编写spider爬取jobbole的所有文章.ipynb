{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通过列表页爬取所有文章的url地址\n",
    "爬取完第一页后，将第二页传递给scrapy，让scrapy自动去爬取第二页\n",
    "1. 改写start_urls\n",
    "    * start_urls = \\['http://blog.jobbole.com/110287/'] -> start_urls = ['http://blog.jobbole.com']\n",
    "2. 改写parse函数\n",
    "    ```\n",
    "    def parse(self, response):\n",
    "    '''\n",
    "        1.获取文章列表页中的文章url并交给解析函数进行具体字段解析\n",
    "        2.获取下一页的url并交给scrapy进行下载\n",
    "    '''\n",
    "    # 解析列表页中的所有文章url并交给scrapy下载后(通过Request函数创建对象交给scrapy)进行解析\n",
    "    # from scrapy.http import Request\n",
    "    post_urls = response.css(\"#archive .floated-thumb .post-thumb a::attr(href)\").extract()\n",
    "    '''\n",
    "    因为在某些网站里的的url是不完整的，也就是没有域名的，没有域名默认当前页面的域名\n",
    "    比如'http://blog.jobbole.com/110287/'写成了'/110287/'，但是提取出href之后是提取不到域名的，\n",
    "    所以需要把当前页面的域名和这个href做一个连接，这才是一个完整的HTTP的url地址\n",
    "    只有当post_url域名完整，才可以使用下面的写法\n",
    "        yield Request(url=post_url, callback=self.parse_detail)\n",
    "    所以考虑这样一个逻辑：\n",
    "    完整域名 = response.url(域名地址) + post_url(href值)\n",
    "    Python提供了一个函数 -> parse.urljoin\n",
    "        from urllib import parse\n",
    "    parse.urljoin(response.url, post_url):\n",
    "        response.url取域名 + post_url取除域名外的部分\n",
    "        例：urljoin('http://www.cwi.nl/Python.html', 'http://www.cwi.nl/FAQ.html')\n",
    "        结果为：'http://www.cwi.nl/FAQ.html'\n",
    "    '''\n",
    "    for post_url in post_urls:\n",
    "        yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail)\n",
    "    \n",
    "    # 提取下一页并交给scrapy进行下载\n",
    "    (此时的回调函数不是parse_detail了，而是parse，因为此时的next_url为列表页，不是详情页)\n",
    "    next_url = response.css('.next.page-numbers::attr(href)').extract()\n",
    "    if next_url:\n",
    "        yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse)\n",
    "\n",
    "    pass\n",
    "    ```\n",
    "3.增添parse_detail函数作为解析函数(回调函数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n def parse_detail(self, response):\\n        # 提取文章具体字段\\n\\n        title = response.xpath(\\'//div[@class=\"entry-header\"]/h1/text()\\').extract()\\n\\n        create_time = response.xpath(\\'//p[@class=\"entry-meta-hide-on-mobile\"]/text()\\').extract()[0].             strip().replace(\\'·\\', \\'\\').strip()\\n\\n        praise_num = int(response.xpath(\"//span[contains(@class, \\'vote-post-up\\')]/h10/text()\").extract()[0])\\n\\n        fav_num = response.xpath(\"//span[contains(@class, \\'bookmark-btn\\')]/text()\").extract()[0]\\n        match_re = re.match(\\'.*?(\\\\d+).*\\', fav_num)\\n        if match_re:\\n            fav_num = match_re.group(1)\\n\\n        comments_num = response.xpath(\"//a[@href=\\'#article-comment\\']/span/text()\").extract()[0]\\n        match_re = re.match(\\'.*?(\\\\d+).*\\', comments_num)\\n        if match_re:\\n            comments_num = match_re.group(1)\\n\\n        tag_list = response.xpath(\\'//p[@class=\"entry-meta-hide-on-mobile\"]/a/text()\\').extract()\\n        tag_list = [element for element in tag_list if not element.strip().endswith(\\'评论\\')]\\n        tags = \\',\\'.join(tag_list)\\n        pass\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " def parse_detail(self, response):\n",
    "        # 提取文章具体字段\n",
    "\n",
    "        title = response.xpath('//div[@class=\"entry-header\"]/h1/text()').extract()\n",
    "\n",
    "        create_time = response.xpath('//p[@class=\"entry-meta-hide-on-mobile\"]/text()').extract()[0]. \\\n",
    "            strip().replace('·', '').strip()\n",
    "\n",
    "        praise_num = int(response.xpath(\"//span[contains(@class, 'vote-post-up')]/h10/text()\").extract()[0])\n",
    "\n",
    "        fav_num = response.xpath(\"//span[contains(@class, 'bookmark-btn')]/text()\").extract()[0]\n",
    "        match_re = re.match('.*?(\\d+).*', fav_num)\n",
    "        if match_re:\n",
    "            fav_num = match_re.group(1)\n",
    "        else:\n",
    "            fav_num = 0\n",
    "\n",
    "        comments_num = response.xpath(\"//a[@href='#article-comment']/span/text()\").extract()[0]\n",
    "        match_re = re.match('.*?(\\d+).*', comments_num)\n",
    "        if match_re:\n",
    "            comments_num = match_re.group(1)\n",
    "        else:\n",
    "            comments_num = 0\n",
    "        \n",
    "        tag_list = response.xpath('//p[@class=\"entry-meta-hide-on-mobile\"]/a/text()').extract()\n",
    "        tag_list = [element for element in tag_list if not element.strip().endswith('评论')]\n",
    "        tags = ','.join(tag_list)\n",
    "        pass\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "response.css(\"#archive .floated-thumb .post-thumb a::attr(href)\").extract()  \n",
    "Out[4]:\n",
    "['http://blog.jobbole.com/114321/',\n",
    " 'http://blog.jobbole.com/114319/',\n",
    " 'http://blog.jobbole.com/114311/',\n",
    " 'http://blog.jobbole.com/114308/',\n",
    " 'http://blog.jobbole.com/114303/',\n",
    " 'http://blog.jobbole.com/114297/',\n",
    " 'http://blog.jobbole.com/114285/',\n",
    " 'http://blog.jobbole.com/114283/',\n",
    " 'http://blog.jobbole.com/114280/',\n",
    " 'http://blog.jobbole.com/114276/',\n",
    " 'http://blog.jobbole.com/114273/',\n",
    " 'http://blog.jobbole.com/114270/',\n",
    " 'http://blog.jobbole.com/114268/',\n",
    " 'http://blog.jobbole.com/114261/',\n",
    " 'http://blog.jobbole.com/114168/',\n",
    " 'http://blog.jobbole.com/114256/',\n",
    " 'http://blog.jobbole.com/114253/',\n",
    " 'http://blog.jobbole.com/114250/',\n",
    " 'http://blog.jobbole.com/114167/',\n",
    " 'http://blog.jobbole.com/114241/']\n",
    " ___\n",
    " ？如何获取下一页\n",
    "\n",
    "<img src='14.png' width=500>\n",
    "<img src='15.png' width=500>\n",
    "\n",
    "next_url = response.css('.next.page-numbers::attr(href)').extract()  \n",
    "Out[10]: ['http://blog.jobbole.com/all-posts/page/2/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrapy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3868429a7597>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mJobboleSpider\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscrapy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSpider\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'jobbole'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mallowed_domains\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'blog.jobbole.com'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstart_urls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'http://blog.jobbole.com/all-posts/'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scrapy' is not defined"
     ]
    }
   ],
   "source": [
    "class JobboleSpider(scrapy.Spider):\n",
    "    name = 'jobbole'\n",
    "    allowed_domains = ['blog.jobbole.com']\n",
    "    start_urls = ['http://blog.jobbole.com/all-posts/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "        1.获取文章列表页中的文章url并交给解析函数进行具体字段解析\n",
    "        2.获取下一页的url并交给scrapy进行下载\n",
    "\n",
    "        :param response:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print(response.url)\n",
    "        # 获取文章列表页中的文章url并交给解析函数(parse_detail)进行具体字段解析\n",
    "        post_urls = response.css(\"#archive .floated-thumb .post-thumb a::attr(href)\").extract()\n",
    "        for post_url in post_urls:\n",
    "            print(response.url + post_url)\n",
    "            yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail)\n",
    "\n",
    "        # 提取下一页并交给scrapy进行下载(此时的回调函数不是parse_detail了，而是parse，因为此时的next_url为列表页，不是详情页)\n",
    "        next_url = response.css('.next.page-numbers::attr(href)').extract()\n",
    "        if next_url:\n",
    "            yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse)\n",
    "\n",
    "    def parse_detail(self, response):\n",
    "        # 提取文章具体字段\n",
    "\n",
    "        title = response.xpath('//div[@class=\"entry-header\"]/h1/text()').extract()\n",
    "\n",
    "        create_time = response.xpath('//p[@class=\"entry-meta-hide-on-mobile\"]/text()').extract()[0]. \\\n",
    "            strip().replace('·', '').strip()\n",
    "\n",
    "        praise_num = int(response.xpath(\"//span[contains(@class, 'vote-post-up')]/h10/text()\").extract()[0])\n",
    "\n",
    "        fav_num = response.xpath(\"//span[contains(@class, 'bookmark-btn')]/text()\").extract()[0]\n",
    "        match_re = re.match('.*?(\\d+).*', fav_num)\n",
    "        if match_re:\n",
    "            fav_num = int(match_re.group(1))\n",
    "        else:\n",
    "            fav_num = 0\n",
    "\n",
    "        comments_num = response.xpath(\"//a[@href='#article-comment']/span/text()\").extract()[0]\n",
    "        match_re = re.match('.*?(\\d+).*', comments_num)\n",
    "        if match_re:\n",
    "            comments_num = int(match_re.group(1))\n",
    "        else:\n",
    "            comments_num = 0\n",
    "\n",
    "        tag_list = response.xpath('//p[@class=\"entry-meta-hide-on-mobile\"]/a/text()').extract()\n",
    "        tag_list = [element for element in tag_list if not element.strip().endswith('评论')]\n",
    "        tags = ','.join(tag_list)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scrapy",
   "language": "python",
   "name": "scrapy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
