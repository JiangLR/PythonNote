{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Selenium 是一个用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，就像真正的用户在操作一样\n",
    "2. 使用Selenium必须先安装浏览器对应的Selenium驱动\n",
    "    * pip install selenium\n",
    "    * 在Selenium官网下载对应的驱动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from selenium import webdriver\n",
    "browser = webdriver.Chrome(executable_path=\"D:/Temp/chromedriver.exe\")\n",
    "browser.get(\"https://www.zhihu.com/#signin\")\n",
    "print(browser.page_source)\n",
    "brower.quit()\n",
    "```\n",
    "* browser.page_source:获取加载JS代码之后的网页\n",
    "* brower.quit():关闭浏览器\n",
    "\n",
    "1. Selenium提供了Selector来选取页面元素\n",
    "    * 例如browser.find_element_by_css_selector...\n",
    "    * 不推荐使用Selnium提供的选择器,因为Selnium是用纯python写的,速度肯定是比不上用Scrapy,因为Scrapy中的Selector是用C写的LXML来完成的.如果只是选择页面中国的某些元素,则需要使用Scrapy的选择器\n",
    "    * 但是想要获取页面中的可点击对象:button,拖拽...,则需要用到Selenium提供的选择器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用Selenium模拟登陆知乎\n",
    "```\n",
    "browser.get(\"https://www.zhihu.com/#signin\")\n",
    "browser.find_element_by_css_selector(\".view-signin input[name='account']\").send_keys(\"18782902568\")\n",
    "browser.find_element_by_css_selector(\".view-signin input[name='password']\").send_keys(\"admin125\")\n",
    "browser.find_element_by_css_selector(\".view-signin button.sign-button\").click()\n",
    "```\n",
    "* send_keys():发送值\n",
    "* click():点击"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### selenium 完成微博模拟登录~\n",
    "```\n",
    "browser.get(\"https://www.oschina.net/blog\")\n",
    " import time\n",
    " time.sleep(5)\n",
    " browser.find_element_by_css_selector(\"#loginname\").send_keys(\"liyao198705@sina.com\")\n",
    " browser.find_element_by_css_selector(\".info_list.password input[node-type='password']\").send_keys(\"da_ge_da\")\n",
    " browser.find_element_by_css_selector(\".info_list.login_btn a[node-type='submitBtn']\").click()\n",
    "\n",
    " for i in range(3):\n",
    "     browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight); var lenOfPage=document.body.scrollHeight; return lenOfPage;\")\n",
    "     time.sleep(3)\n",
    " t_selector = Selector(text=browser.page_source)\n",
    " print (t_selector.css(\".tm-promo-price .tm-price::text\").extract())\n",
    "```\n",
    "* execute_script:执行JS代码(实现微博向下滑动)\n",
    "* time.sleep(5):需要给页面加载的延迟时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置chromedriver不加载图片\n",
    "```\n",
    "chrome_opt = webdriver.ChromeOptions()\n",
    "prefs = {\"profile.managed_default_content_settings.images\":2}\n",
    "chrome_opt.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "browser = webdriver.Chrome(executable_path=\"D:/Temp/chromedriver.exe\")\n",
    "browser.get(\"https://www.zhihu.com/#signin\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### selenium集成到scrapy中\n",
    "1. 设置中间件(Middleware),使用selenium说到底也是对发送requests做了另一种方法的实现,所以说可以把selenium逻辑放入Middleware中以此来处理requests\n",
    "\n",
    "```\n",
    "from selenium import webdriver\n",
    "from scrapy.http import HtmlResponse\n",
    "import time\n",
    "class JSPageMiddleware(object):\n",
    "    #通过chrome请求动态网页\n",
    "    def process_request(self, request, spider):\n",
    "        if spider.name == \"jobbole\":\n",
    "            browser = webdriver.Chrome(executable_path=\"D:/Temp/chromedriver.exe\")\n",
    "            browser.get(request.url)\n",
    "            time.sleep(3)\n",
    "            print (\"访问:{0}\".format(request.url))\n",
    "\n",
    "            return HtmlResponse(url=spider.browser.current_url, body=spider.browser.page_source, encoding=\"utf-8\", request=request)\n",
    "```\n",
    "1. 需要注意一点的是Selenium已经帮我们完成对了网页的访问,我们没必要再把Request发送给DOWNLOADER再去重复访问网页.因此我们需要在DOWNLOADERMIDDLEWARE处截断Request向DOWNLOADER发送请求\n",
    "2. 处理截断请求的方式就是直接返回一个Response,\n",
    "    * from scrapy.http import HtmlResponse\n",
    "    * HtmlResponse(url, body, encoding, request)\n",
    "3. 别忘了在setting.py设置middleware\n",
    "4. 得到了返回值(HtmlResponse)就可以被Spider.py文件里的对应Request()所得到的\n",
    "5. **但是现在有一个非常大的问题:每次请求一个页面的时候都会去打开一个浏览器,这是非常慢的**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 改版一:\n",
    "```\n",
    "from selenium import webdriver\n",
    "from scrapy.http import HtmlResponse\n",
    "class JSPageMiddleware(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.browser = webdriver.Chrome(executable_path=\"D:/Temp/chromedriver.exe\")\n",
    "        super(JSPageMiddleware, self).__init__()\n",
    "    \n",
    "    #通过chrome请求动态网页\n",
    "    def process_request(self, request, spider):\n",
    "        if spider.name == \"jobbole\":\n",
    "            self.browser.get(request.url)\n",
    "            import time\n",
    "            time.sleep(3)\n",
    "            print (\"访问:{0}\".format(request.url))\n",
    "\n",
    "            return HtmlResponse(url=self.browser.current_url, body=self.browser.page_source, encoding=\"utf-8\", request=request)\n",
    "```\n",
    "1. 在类中初始化一个browser,所以在每次请求时就不用创建浏览器了(所有请求共用一个browser)\n",
    "2. 但是现在还有一个问题就是当处理Spider任务时,Spider会关闭,但是浏览器却不会关闭\n",
    "    * 问题在于Midelware中提供的只有以下四种方法,没有提供在process_request()中关闭Spider的方法\n",
    "        * process_response(request, response, spider)\n",
    "        * process_request(request, spider)\n",
    "        * process_exception(request, exception, spider)\n",
    "        * from_crawler(cls, crawler)\n",
    "3. 鉴于以上三种方法的参数都有spider,可以考虑在spider里创建浏览器,这样每一个Spider都只需要对应一个浏览器即可.同时既然想实现在Spider结束任务时关闭浏览器的逻辑,Scrapy提供了**Signals(信号)**的机制,当Spider结束任务时会发送一个信号,我们可以在这个信号方法中定义我们想要实现的逻辑  \n",
    "\n",
    "#### 改版二:\n",
    "```\n",
    "from scrapy.xlib.pydispatch import dispatcher\n",
    "from scrapy import signals\n",
    "\n",
    "class JobboleSpider(scrapy.Spider):\n",
    "    name = \"jobbole\"\n",
    "    allowed_domains = [\"blog.jobbole.com\"]\n",
    "    start_urls = ['http://blog.jobbole.com/all-posts/']\n",
    "\n",
    "    def __init__(self):\n",
    "         self.browser = webdriver.Chrome(executable_path=\"D:/Temp/chromedriver.exe\")\n",
    "         super(JobboleSpider, self).__init__()\n",
    "         dispatcher.connect(self.spider_closed, signals.spider_closed)\n",
    "\n",
    "    def spider_closed(self, spider):\n",
    "         #当爬虫退出的时候关闭chrome\n",
    "         print (\"spider closed\")\n",
    "         self.browser.quit()\n",
    "```\n",
    "* from scrapy.xlib.pydispatch import dispatcher:**dispatcher**是一个分发器,设置了当某个信号量发生的时候需要被加载的函数\n",
    "* from scrapy import signals:这一行导入是必须的\n",
    "\n",
    "* 与此相对应过的Middleware中的逻辑代码就不需要初始化建立webdriver了,webdriver通过spider传递:\n",
    "```\n",
    "class JSPageMiddleware(object):\n",
    "\n",
    "    #通过chrome请求动态网页\n",
    "    def process_request(self, request, spider):\n",
    "        if spider.name == \"jobbole\":\n",
    "            # browser = webdriver.Chrome(executable_path=\"D:/Temp/chromedriver.exe\")\n",
    "            spider.browser.get(request.url)\n",
    "            import time\n",
    "            time.sleep(3)\n",
    "            print (\"访问:{0}\".format(request.url))\n",
    "\n",
    "            return HtmlResponse(url=spider.browser.current_url, body=spider.browser.page_source, encoding=\"utf-8\", request=request)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scrapy",
   "language": "python",
   "name": "scrapy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
