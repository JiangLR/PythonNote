{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### User Agent中文名为用户代理，简称 UA，它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本、浏览器渲染引擎、浏览器语言、浏览器插件等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. middlewares有两种,根据Scrapy架构图,middleware不仅能处理request也能处理response:\n",
    "    * SPIDER_MIDDLEWARES\n",
    "    * DOWNLOADER_MIDDLEWARES\n",
    "2. 在使用middleware之前,需要在settings.py里配置\n",
    "```\n",
    "SPIDER_MIDDLEWARES = {\n",
    "    'ArticleSpider.middlewares.ArticlespiderSpiderMiddleware': 543,\n",
    "}\n",
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'ArticleSpider.middlewares.ArticlespiderDownloaderMiddleware': 543,\n",
    "}\n",
    "```\n",
    "    * 这些配置和之前pipeline的配置一样,数字代表执行顺序,数字越小执行顺序越靠前\n",
    "    * 可以自定义middleware\n",
    "3. middleware的书写格式\n",
    "    * scrapy自带有一个关于useragent的middleware(useragent.py),可以参考这个代码来自定义自己的middleware\n",
    "    ```\n",
    "    class UserAgentMiddleware(object):\n",
    "    \"\"\"This middleware allows spiders to override the user_agent\"\"\"\n",
    "\n",
    "    def __init__(self, user_agent='Scrapy'):\n",
    "        self.user_agent = user_agent\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        o = cls(crawler.settings['USER_AGENT'])\n",
    "        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n",
    "        return o\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        self.user_agent = getattr(spider, 'user_agent', self.user_agent)\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        if self.user_agent:\n",
    "            request.headers.setdefault(b'User-Agent', self.user_agent)\n",
    "    ```\n",
    "    * 之前笔记中的def from_crawler(cls, crawler)又出现了一次,这次是定义在Middleware中,这里的from_crawler读取了crawler里setting定义的USER_AGENT,若USER_AGENT没有在setting.py中被定义,则会被赋值为'Scrapy'(user_agent='Scrapy'),所以输可以在setting.py中设置USER_AGENT\n",
    "    * def process_request(self, request, spider)是一个非常重要的函数,有固定的写法.在这个方法中处理request\n",
    "    * 在配置DOWNLOADER_MIDDLEWARES/SPIDER_MIDDLEWARES要记得现在dict里把默认的middleware置为None或者把自己自定义的middleware的顺序数字调高,因为默认的middleware会把user-agent置为'Scrapy',如果说默认的middleware执行顺序在自定义middleware之后,就会把我们自定义的规则覆盖掉.所以要么置为None,要么就把默认middleware的执行数字调第,保证自定义的middleware执行顺序在默认middleware执行顺序之后\n",
    "        * 默认的middleware为:'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None\n",
    "    * middleware的主要函数为:\n",
    "        * process_response(request, response, spider)\n",
    "        * process_request(request, spider)\n",
    "        * process_exception(request, exception, spider)\n",
    "        * from_crawler(cls, crawler)\n",
    "4. 自定义middleware(**以下代码只是显示自定义middleware该如何书写,但对于处理user-agent还有可以优化的地方**)\n",
    "```\n",
    "class RandomUserAgentMiddlware(object):\n",
    "    #随机更换user-agent\n",
    "    def __init__(self, crawler):\n",
    "        super(RandomUserAgentMiddlware, self).__init__()\n",
    "        self.user_agent_list = crawler.settings.get(\"user_agent_list\", [])\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(crawler)\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        def get_ua():\n",
    "            return getattr(self.ua, self.ua_type)\n",
    "\n",
    "        request.headers.setdefault('User-Agent', self.user_agent_list[random.randInt(len(user_agent_list))])\n",
    "```\n",
    "5. 优化处理user-agent的middleware\n",
    "    * 在github上有一个专门随机获取user-agent的开源代码'fake-useragent'\n",
    "```\n",
    "class RandomUserAgentMiddlware(object):\n",
    "    #随机更换user-agent\n",
    "    def __init__(self, crawler):\n",
    "        super(RandomUserAgentMiddlware, self).__init__()\n",
    "        self.ua = UserAgent()\n",
    "        # 需要在settings.py中设置\"RANDOM_UA_TYPE\"=\"random\"\n",
    "        self.ua_type = crawler.settings.get(\"RANDOM_UA_TYPE\", \"random\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(crawler)\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        def get_ua():\n",
    "            return getattr(self.ua, self.ua_type)\n",
    "\n",
    "        request.headers.setdefault('User-Agent', get_ua())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scrapy",
   "language": "python",
   "name": "scrapy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
