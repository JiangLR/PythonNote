{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='1.png' width=800px>\n",
    "### Scrapy核心组件:\n",
    "1. **引擎(Scrapy engine)**: 用来处理整个系统的数据流处理, 触发事务(框架核心)\n",
    "2. **调度器(Scheduler)**: 用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址\n",
    "3. **下载器(Downloader)**: 用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)\n",
    "4. **爬虫(Spiders)**: 爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面\n",
    "5. **项目管道(Pipeline)**: 负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。\n",
    "6. **下载器中间件(Downloader Middlewares)**: 位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。\n",
    "7. **爬虫中间件(Spider Middlewares)**: 介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。\n",
    "\n",
    "\n",
    "### Crawler\n",
    "1. Crawler(爬虫):是Scrapy API的主要入口点,这个对象提供了对所有Scrapy核心组件的访问\n",
    "2. Crawler是跟Scrapy的settings挂钩起来的,一般的组件类中都有from_crawler()这个方法\n",
    "3. **from_crawler**用于实例化某个对象（中间件，模块），常常出现在对象的初始化，负责提供crawler.settings\n",
    "\n",
    "##### 比如Spider类中有from_crawler(),通过绑定crawler来初始化Spider对象,也就是说Spider有了读取setting的能力\n",
    "```\n",
    "class Spider(object_ref):\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler, *args, **kwargs):\n",
    "        spider = cls(*args, **kwargs)\n",
    "        spider._set_crawler(crawler)\n",
    "        return spider\n",
    "\n",
    "    def _set_crawler(self, crawler):\n",
    "        self.crawler = crawler\n",
    "        self.settings = crawler.settings\n",
    "        crawler.signals.connect(self.close, signals.spider_closed)\n",
    "```\n",
    "##### 再比如PipeLine\n",
    "**MediaPipeline是Pipeline的初始父类**\n",
    "```\n",
    "class MediaPipeline(object):\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        try:\n",
    "            pipe = cls.from_settings(crawler.settings)\n",
    "        except AttributeError:\n",
    "            pipe = cls()\n",
    "        pipe.crawler = crawler\n",
    "        return pipe\n",
    "```\n",
    "**cls.from_settings(crawler.settings)就读取了crawler的配置**\n",
    "##### 再比如Middleware\n",
    "```\n",
    "class DepthMiddleware(object):\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        settings = crawler.settings\n",
    "        maxdepth = settings.getint('DEPTH_LIMIT')\n",
    "        verbose = settings.getbool('DEPTH_STATS_VERBOSE')\n",
    "        prio = settings.getint('DEPTH_PRIORITY')\n",
    "        return cls(maxdepth, crawler.stats, verbose, prio)\n",
    "```\n",
    "**settings = crawler.settings就读取了crawler的配置**  \n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从上面的流程图来分析Spider类中yield()方法\n",
    "```\n",
    "class ZhihuSpider(scrapy.Spider):\n",
    "    name = 'zhihu'\n",
    "    allowed_domains = ['www.zhihu.com']\n",
    "    start_urls = ['http://www.zhihu.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        yield scrapy.Request(request_url, headers=self.headers, callback=self.parse_question)\n",
    "\n",
    "    def parse_question(self, response):\n",
    "        item_loader = ItemLoader(item=ZhihuQuestionItem(), response=response)\n",
    "        item_loader.add_css(\"title\", \".zh-question-title h2 a::text\")\n",
    "        question_item = item_loader.load_item()\n",
    "        \n",
    "        yield scrapy.Request(self.start_answer_url.format(question_id, 20, 0), headers=self.headers,\n",
    "                             callback=self.parse_answer)\n",
    "        yield question_item\n",
    "\n",
    "    def parse_answer(self, response):\n",
    "        for answer in ans_json[\"data\"]:\n",
    "            answer_item = ZhihuAnswerItem()\n",
    "            yield answer_item\n",
    "\n",
    "    def start_requests(self):\n",
    "        return [scrapy.Request('https://www.zhihu.com/#signin', headers=self.headers, callback=self.login)]\n",
    "\n",
    "    def login(self, response):\n",
    "        return [scrapy.FormRequest()]\n",
    "\n",
    "    def check_login(self, response):\n",
    "        yield scrapy.Request(url, dont_filter=True, headers=self.headers)\n",
    "```\n",
    "1. Spider类只有发送request和接受response的功能,所以能看到\n",
    "    * 只有**start_requests(self):**的参数中是没有response的,因为start_requests(self)负责发送request\n",
    "    * 而其他方法都有response参数,他们负责接受response,对应流程图中是**⑥**\n",
    "2. 关键的一步,所有方法的yield()都对应**⑦**,当Scrapy engine接受到item或者Request时会执行**⑧**\n",
    "    * 可以看到def parse_question(self, response)方法产生了两个yield,也就是说在**⑧**能同时往两个方向进行,即同时发送item给PipeLine和发送Request给调度器"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scrapy",
   "language": "python",
   "name": "scrapy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
