{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 在Scrapy目录中创建关于知乎的爬虫\n",
    "    * scrapy genspider zhihu www.zhihu.com\n",
    "2. 知乎在未登录的状态下不能访问其中的页面,所以在爬取知乎数据之前需要完成登录操作\n",
    "    * 需要重载**scrapy.Spider**的中的**start_requests()** 函数(\n",
    "        * 所有的url本质上初始时都会被start_requests()处理,只是如果不主动覆写start_requests()的话就使用默认的方法,在start_requests()里通过调用yield把处理结果送给pipline.py里继续处理\n",
    "        * **start_requests()**:此方法用于生成初始请求,他**必须返回一个可迭代的对象(yield x ; return [x])**.此方法会**默认使用start_urls里面的URL来构造Request**,而且Request是GET请求方式.如果我们想在启动时以POST方式来访问某个站点,可以直接重写这个方法,**发送POST请求时使用scrapy.FormRequest**即可\n",
    "            ```\n",
    "        def start_requests(self):\n",
    "            if method_is_overridden(cls, Spider, 'make_requests_from_url'):\n",
    "                for url in self.start_urls:\n",
    "                    yield self.make_requests_from_url(url)\n",
    "            else:\n",
    "                for url in self.start_urls:\n",
    "                    yield Request(url, dont_filter=True)\n",
    "            ```\n",
    "            * **scrapy.FormRequest**的参数跟Request()的参数相同(headers,url,data,callback...),只是其中有个叫**formdata**的参数,实际上就是Request()中的data\n",
    "        * 由上可知重写start_requests()原因有几点:\n",
    "            * scrapy默认的start_urls是固定的的,现在假设有这么个需求：爬虫需要先从数据库里面读取目标URL再依次进行爬取，这时候固定的start_urls就显得不够灵活了\n",
    "            * scrapy中Request默认的请求方式是GET,如果我们需要用POST请求就得重写start_requests()\n",
    "            * 重写start_requests()可以满足自定义的需求(比如在Request()中传入params,headers...),这些是默认start_requests()无法做到的\n",
    "    * **只有当访问完页面时才会调用callback函数**,回调函数接收访问完页面时返回的response \n",
    "    * 没有callback(self, response)函数时会默认调用**parse(self, response)**方法,这也解释了为什么如果没有重写start_requests()的话会去调用parse(),因为默认的start_requests()中的Request()没有callback函数\n",
    "    * 使用scrapy时不需要使用session.cookie.save()来保存cookie值,因为scrapy自动将cookie放在Request()中并持续整个爬取流程,所以当我们请求完一个URL时直接可以请求下一个URL,只要我们一开始就获取了cookie值(保存有个人登录信息),之后跟个人信息有关的所有页面都能够访问\n",
    "            \n",
    "    ```\n",
    "    class ZhihuSpider(scrapy.Spider):\n",
    "        name = 'zhihu'\n",
    "        allowed_domains = ['www.zhihu.com']\n",
    "        start_urls = ['http://www.zhihu.com/']\n",
    "        headers = {\n",
    "            \"HOST\": \"www.zhihu.com\",\n",
    "            \"Referer\": \"https://www.zhizhu.com\",\n",
    "            'User-Agent': \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0\"\n",
    "        }\n",
    "\n",
    "        def start_requests(self):\n",
    "            pass\n",
    "    ```\n",
    "    ```\n",
    "    def start_requests(self):\n",
    "        return [scrapy.Request('https://www.zhihu.com/#signin', headers=self.headers, callback=self.login)]\n",
    "    ```\n",
    "    ```\n",
    "    def login(self, response):\n",
    "        response_text = response.text\n",
    "        match_obj = re.match('.*name=\"_xsrf\" value=\"(.*?)\"', response_text, re.DOTALL)\n",
    "        xsrf = ''\n",
    "        if match_obj:\n",
    "            xsrf = (match_obj.group(1))\n",
    "\n",
    "        if xsrf:\n",
    "            post_url = \"https://www.zhihu.com/login/phone_num\"\n",
    "            post_data = {\n",
    "                \"_xsrf\": xsrf,\n",
    "                \"phone_num\": \"18782902568\",\n",
    "                \"password\": \"admin123\"\n",
    "            }\n",
    "\n",
    "            return [scrapy.FormRequest(\n",
    "                url = post_url,\n",
    "                formdata = post_data,\n",
    "                headers=self.headers,\n",
    "                callback=self.check_login\n",
    "            )]\n",
    "    ```\n",
    "    ```\n",
    "    def check_login(self, response):\n",
    "        #验证服务器的返回数据判断是否成功\n",
    "        text_json = json.loads(response.text)\n",
    "        if \"msg\" in text_json and text_json[\"msg\"] == \"登录成功\":\n",
    "            for url in self.start_urls:\n",
    "                yield scrapy.Request(url, dont_filter=True, headers=self.headers)\n",
    "    ```\n",
    "    ##### 所谓的URL去重，就是爬虫将重复抓取的URL去除，避免多次抓取同一网页。\n",
    "    **dont_filter=True**表示可以对重复的URL地址进行爬取\n",
    "    ##### 登陆成功后再次访问 http://www.zhihu.com 即可成功,因为check_login()中的Request()没有指定callback,所以在parse()中进行后续操作"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scrapy",
   "language": "python",
   "name": "scrapy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
