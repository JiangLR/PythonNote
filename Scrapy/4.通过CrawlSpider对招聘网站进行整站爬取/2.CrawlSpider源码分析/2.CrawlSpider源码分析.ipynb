{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 自动创建scrapy的命令为scrapy genspider,但是这只是基于基础模板(basic)创建的爬虫\n",
    "2. **scrapy genspider --list**可以列出所有可创建的模板<img src='1.png'>\n",
    "3. 基于crawl模板创建爬虫\n",
    "    * scrapy genspider -t crawl lagou www.lagou.com\n",
    "\n",
    "```\n",
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "\n",
    "class LagouSpider(CrawlSpider):\n",
    "    name = 'lagou'\n",
    "    allowed_domains = ['www.lagou.com']\n",
    "    start_urls = ['http://www.lagou.com/']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),\n",
    "    )\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        i = {}\n",
    "        # i['domain_id'] = response.xpath('//input[@id=\"sid\"]/@value').extract()\n",
    "        # i['name'] = response.xpath('//div[@id=\"name\"]').extract()\n",
    "        # i['description'] = response.xpath('//div[@id=\"description\"]').extract()\n",
    "        return i\n",
    "```\n",
    "\n",
    "##### CrawlSpider:常用的爬取网站的Spider, 提供一些规则(Rule)去简单地链接迭代爬取,简单的来说就是对Spider进行了又一次的包装\n",
    "* rules:是一个迭代对象(list/tuple),存放Rule实例\n",
    "* parse_start_url(response)\n",
    "* **不能重写CrawlSpider的parse(),因为parse()方法已经被CralSpider占用了**,但是可以重写**parse_start_url(self, response)**,功能和Spider中的parse()相同\n",
    "    ```\n",
    "    def parse(self, response):\n",
    "        return self._parse_response(response, self.parse_start_url, cb_kwargs={}, follow=True)\n",
    "    ```\n",
    "    ```\n",
    "        def parse_start_url(self, response):\n",
    "        return []\n",
    "    ```\n",
    "    ```\n",
    "        def process_results(self, response, results):\n",
    "        return results\n",
    "    ```\n",
    "* CrawlSpider中的 **_parse_response()** 是核心方法,在 **parse_start_url()** 和 **process_results()** 没有被重写之前, **_parse_response()** 中的第一个if是没有用处的\n",
    "```\n",
    "    def _parse_response(self, response, callback, cb_kwargs, follow=True):\n",
    "        if callback:\n",
    "            cb_res = callback(response, **cb_kwargs) or ()\n",
    "            cb_res = self.process_results(response, cb_res)\n",
    "            for requests_or_item in iterate_spider_output(cb_res):\n",
    "                yield requests_or_item\n",
    "\n",
    "        if follow and self._follow_links:\n",
    "            for request_or_item in self._requests_to_follow(response):\n",
    "                yield request_or_item\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scrapy",
   "language": "python",
   "name": "scrapy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
